
import pandas as pd
import boto3
import json
from sqlalchemy import create_engine

class DataExtractor:
  def extract_from_s3(DataExtractor, df):
   DataExtractor.df = pd.read_json()
   return df
  
  def __init__(self, DataExtractor, read_json, s3):
     
    self.DataExtractor = DataExtractor
    self.read_json = read_json
    self.s3 = s3

bucket = 'daak-bucket'
link = 'https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json.'

# link uploaded to daak-bucket to give a path
# link path in daak-bucket = 's3://daak-bucket/date_details.json'
path = 's3://daak-bucket/date_details.json'

client = boto3.client('s3')

path = 's3://daak-bucket/date_details.json'

df =  pd.read_json(path)

# s3f3 and botocore==1.33.5 were installed respectively to ensure efficient processing

print(df)

df.head()

# cleaning data

# - Drop duplicates 

df = df.drop_duplicates()
df


# removing null values
df = df.replace('null','')
df

df = df.fillna('')
df

# resetting index
df = df.reset_index(drop=True)
df

df.info()

# my database-1 credentials

RDS_HOST = 'database-1.c7f2i0mwhdgu.eu-west-2.rds.amazonaws.com'
RDS_PORT = 5432
RDS_DATABASE ='postgres'
RDS_PASSWORD ='*******'
RDS_USER ='postgres'
DATABASE_TYPE = 'postgresql'
DBAPI = 'pyscopg2'

engine = create_engine(f"{DATABASE_TYPE}://{RDS_USER}:{RDS_PASSWORD}@{RDS_HOST}:{RDS_PORT}/{RDS_DATABASE}")

#  Upload clean data to database with table name dim_date_times.

def upload_to_db(dim_date_times, df):
   df.upload_to_db = dim_date_times.replace
   dim_date_times.replace = dim_date_times
   return df
     
  
df.to_sql('dim_date_times', engine, if_exists='replace')

df



  




















 

















